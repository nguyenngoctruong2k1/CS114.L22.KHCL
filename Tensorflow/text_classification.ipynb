{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenngoctruong2k1/CS114.L22.KHCL/blob/main/Tensorflow/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic4_occAAiAT"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ioaprt5q5US7"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yCl0eTNH5RS3"
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# BÀI TOÁN PHÂN LOẠI VĂN BẢN CƠ BẢN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKY4XMc9o8iB"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/text_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "Hướng dẫn này trình bày phân loại văn bản bắt đầu từ các tệp văn bản thuần túy được lưu trữ trên đĩa.  Bạn sẽ train một bộ phân loại nhị phân để thực hiện phân tích tình cảm trên tập dữ liệu IMDB.  Ở cuối sổ tay, có một bài tập để bạn thử, trong đó bạn sẽ huấn luyện một bộ phân loại đa lớp để dự đoán thẻ cho một câu hỏi lập trình trên Stack Overflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "# gọi thư viện"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-tTFS04dChr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b77facd-fbf2-4259-c936-aa83b10a4c65"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBTI1bi8qdFV"
      },
      "source": [
        "## Phân tích tình cảm\n",
        "\n",
        "Notebook này đào tạo một mô hình phân tích tình cảm để phân loại các bài phê bình phim thành *positive* or *negative*, based on the text of the review. Đây là một ví dụ về *binary* hoặc *two-class—classification*, một loại vấn đề học máy quan trọng và có thể áp dụng rộng rãi.\n",
        "\n",
        "You'll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "### Lấy và tìm hiểu dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJmjM8fR4a2g"
      },
      "source": [
        "#### Lấy dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ZYnuajVlFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6734031-a356-48c8-fc65-0e63b710973c"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoHev4Wp4kDp"
      },
      "source": [
        "#### Vọc với dữ liệu đã lấy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "355CfOvsV1pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502f8855-de66-402e-f7ce-263257e1ba07"
      },
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdb.vocab', 'train', 'test', 'imdbEr.txt', 'README']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ASND15oXpF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2d63c7-2f5e-4a6a-f20a-bfc6216ec190"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train') \n",
        "# Giống như lệnh cd ..\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unsup',\n",
              " 'urls_neg.txt',\n",
              " 'labeledBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'neg',\n",
              " 'pos',\n",
              " 'unsupBow.feat',\n",
              " 'urls_unsup.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysMNMI1CWDFD"
      },
      "source": [
        "`aclImdb/train/pos` và `aclImdb/train/neg` thư mục chứa nhiều tệp văn bản, mỗi tệp là một bài đánh giá phim duy nhất.  Chúng ta hãy nhìn vào một trong số đó."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7g8hFvzWLIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639d53a6-82ce-40a9-eca5-d8f7ae9f9aa7"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk20TEm6ZRFP"
      },
      "source": [
        "### Tải tập dữ liệu\n",
        "\n",
        "Tiếp theo, bạn sẽ tải dữ liệu ra đĩa và chuẩn bị nó thành một định dạng phù hợp để train.  Để làm như vậy, bạn sẽ sử dụng [text_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory) utility, và nó mong đợi được một câu trúc như sau:\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_text_1.txt\n",
        "......a_text_2.txt\n",
        "...class_b/\n",
        "......b_text_1.txt\n",
        "......b_text_2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQauv38Lnok3"
      },
      "source": [
        "Để chuẩn bị một tập dữ liệu cho phân loại nhị phân, bạn sẽ cần hai thư mục trên đĩa, tương ứng với `class_a` và `class_b`. Đây sẽ là những đánh giá tích cực và tiêu cực về phim, có thể tìm thấy trong  `aclImdb/train/pos` và `aclImdb/train/neg`. Vì tập dữ liệu IMDB chứa các thư mục bổ sung, bạn sẽ xóa chúng trước khi sử dụng tiện ích này."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhejsClzaWfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9611657a-2400-42d8-d02f-e8d6ba11e198"
      },
      "source": [
        "# Xóa thư mục unsup trong thư mục train\n",
        "import shutil\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "print(remove_dir)\n",
        "shutil.rmtree(remove_dir, ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./aclImdb/train/unsup\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95kkUdRoaeMw"
      },
      "source": [
        "- Tiếp theo, sử dụng `text_dataset_from_directory` là hàm để tạo `tf.data.Dataset`. [tf.data](https://www.tensorflow.org/guide/data) là một tập hợp các công cụ mạnh mẽ làm việc với dữ liệu.\n",
        "\n",
        "- Khi chạy thử một chương trình máy học, cách tốt nhất là chia dữ liệu thành 3 phần: [train](https://developers.google.com/machine-learning/glossary#training_set), [validation](https://developers.google.com/machine-learning/glossary#validation_set), và [test](https://developers.google.com/machine-learning/glossary#test-set). \n",
        "\n",
        "- Tập dữ liệu IMDB đã được chia thành huấn luyện và kiểm tra, nhưng nó thiếu tập hợp xác thực.  Hãy tạo một tập hợp xác thực bằng cách sử dụng phân tách 80:20 của dữ liệu huấn luyện bằng cách sử dụng đối số validation_split bên dưới."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6PS8FJRRzBp"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOrK-MTYaw3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079319f1-f43c-48e5-ac0c-e56b0a23602a"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)\n",
        "# print(raw_train_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y33oxOUpYkh"
      },
      "source": [
        "Như bạn có thể thấy ở trên, có 25,000 ví dụ trong thư mục đào tạo, trong đó bạn sẽ sử dụng 80% (hoặc 20.000) để đào tạo.  Như bạn sẽ thấy trong giây lát, bạn có thể đào tạo một mô hình bằng cách chuyển trực tiếp tập dữ liệu tới `model.fit`. Nếu bạn là người mới `tf.data`, bạn cũng có thể lặp lại tập dữ liệu và in ra một vài ví dụ như sau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51wNaPPApk1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3c0dbd-3a56-418c-881a-b22f53ece25a"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(2):\n",
        "  for i in range(3):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])\n",
        "print(raw_train_ds.take(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review b'Belmondo is a tough cop. He goes after a big-time drug dealer (played by Henry Silva, normally a great villain - see \"Sharky\\'s Machine\"; but here he is clearly dubbed, and because of that he lacks his usual charisma). He goes to the scuzziest places of Paris and Marseilles, asks for some names, beats up some people, gets the names, goes to more scuzzy places, asks for more names, beats up more people, etc. The whole movie is punch after punch after punch. It seems that the people who made it had no other ambition than to create the French equivalent of \"Dirty Harry\". Belmondo, who was 50 here, does perform some good stunts at the beginning; apart from those, \"Le Marginal\" is a violent, episodic, trite, shallow and forgettable cop movie. (*1/2)'\n",
            "Label 0\n",
            "Review b'Wow. The only people reviewing this positively are the Carpenter apologists. I know a lot of those. The guys that\\'ll watch John Carpenter squat on celluloid and pinch out a movie and proclaim it a masterwork of horror. This \"movie\" is utter crap. It looks and sounds like a porno (good lord, the soundtrack is awful...), and has sub-par porn acting, which is shocking, because normally Ron Perlman is really a very good actor. I honestly have no idea what Carpenter was thinking when making this. Most likely \"Beans, beans, beans..\" until somebody fed him and rolled him up into a blanket for the day... They say nothing about the abortion debate whatsoever, when they could have had a very interesting central theme (how do religious zealot anti-abortionists feel when it\\'s the devil\\'s baby?) but instead they chose to have Ron Perlman and his terribly acted kids kill a bunch of people and have the horribly cast doctors try to calm the hysterically bad pregnant girl. Not a single person from this episode or what have you should come away unscathed. It\\'s just awful. Like, Plan 9 From Outerspace awful. Like, good god please would somebody turn it off before I soil myself awful. Try watching this and The Thing in the same day and your mind will implode.'\n",
            "Label 0\n",
            "Review b'Even though it has one of the standard \"Revenge Price Plots,\" this film is my favorite of Vincent Price\\'s work. Gallico has that quality that is missing in so many horror film characters- likeability. When you watch it, you feel for him, you feel his frustration, the injustices against him, and you cheer him on when he goes for vengeance, even though he frightens you a little with his original fury. As the film goes on, his character becomes tragic. He\\'s committed his murder, but now he must kill to cover that up. And again to cover that one up. And again... your stomach sinks with his soul as it goes down its spiral- like watching a beloved brother turn into a hood. Even if the revenge story is of old, the plot devices themselves are original- Gallico uses his tricks to kill in more and more inventive ways. A shame this one isn\\'t available for home veiwing.'\n",
            "Label 1\n",
            "Review b'Many people here say that this show is for kids only. Hm, when I was a kid (approximately 7-9 years old) I watched this show first. It was disgusting for me. I talked with other kids about this and, sure, other shows and know what? This was the measure of disguise, whenever we wanted to emphasize something\\'s silliness (either on TV or anything else) we said \"Uh, just like Power Rangers\" and laughed. <br /><br />And before visiting this site I could not imagine that there actually are fans of MMPR. It was so strange for me that I decided to watch it again and try to understand why people like it. I did not enjoy that viewing. But it dawned upon me: maybe I have not enough imagination? It may be. However this argument is not sufficient for me to rate it more than 1 star.'\n",
            "Label 0\n",
            "Review b\"This, I think, is one of the best pictures ever made. It's so pure and beautiful. It really touched me. I'm glad David Lynch proved that a film doesn't necessarily need SFX, a twisting, complicated plot or flashy images. Way to go,Dave. I'd like to see Cronenberg do that!\"\n",
            "Label 1\n",
            "Review b'This movie was on the Romance channel, and I thought it might be a goofy 80\\'s movie that would be enjoyable on some level, so my brother and I watched it. Boy did it suck. Boy gets crush on girl--correction, his *dream*-girl (apparently there is a difference; and I\\'m surprised he realized she was his dream girl--he was smitten with her from over 30 feet away. I guess that just goes to show the power of dream-girls), boy ends up masquerading as a female to be near dream-girl (creative in the sense that it\\'s a far-out plan, but un-creative in the sense that there are probably better solutions one might think up), awkward situations ensue, a match is made (all of which takes seems to take place around late afternoon--either the location was somehow responsible for this odd lighting, or the actors had to wait until they got off of their day-jobs to come to the set; I suspect the latter). Very clumsily done, very pathetic. It\\'s almost never even amusing *accidentally*, so there really is nothing to redeem it. Unless you\\'re interested in seeing Chad Lowe\\'s early days, before he finally got his piece of the pie with his role as the HIV-positive gay guy on the series \"Life Goes On\", or Gail O\\'Grady who was on NYPD Blue and probably got to stare at Dennis Franz\\'s buttocks). But those are unlikely motives--I\\'d say \"systematic derangement of the senses\" would be a more justified purpose. I\\'m surprised I watched it all. I guess it\\'s the kind of thing where, halfway through, you find yourself *still* watching due to some morbid, self-flagellistic inner-issue, and think you might as well finish it so you can tell your friends and family that you actually sat through such a horrible movie, on the off-chance that it\\'ll garner you some sympathy for the questionable state of your mental health. Can *You* Take the Challenge?'\n",
            "Label 0\n",
            "<TakeDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWq1SUIrp1a-"
      },
      "source": [
        "Lưu ý rằng các bài đánh giá có chứa văn bản thô (với dấu chấm câu và các thẻ HTML không thường xuyên như `<br/>`).  Bạn sẽ chỉ ra cách xử lý những điều này trong phần sau.\n",
        "\n",
        "Các nhãn là 0 hoặc 1. Để xem nhãn nào trong số này tương ứng với các đánh giá tích cực và tiêu cực về phim, bạn có thể kiểm tra thuộc tính `class_names` trên tập dữ liệu.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlICTG8spyO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cedbdcb-9526-49dc-ed54-70def7385036"
      },
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to neg\n",
            "Label 1 corresponds to pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbdO39vYqdJr"
      },
      "source": [
        "Tiếp theo, bạn sẽ tạo một tập dữ liệu xác nhận và kiểm tra.  Bạn sẽ sử dụng 5.000 đánh giá còn lại từ tập huấn luyện để xác nhận."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzxazN8Hq1pF"
      },
      "source": [
        "Lưu ý: Khi sử dụng các đối số `validation_split` và `subset`, hãy đảm bảo chỉ định một hạt ngẫu nhiên hoặc chuyển `shuffle = False`, để phân tách xác thực và đào tạo không có sự chồng chéo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsMwwhOoqjKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00c9018-dceb-4a93-f44b-903ea8aace5a"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdSr0Nt3q_ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9765a85-b0e6-4594-8419-718556978121"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDA_Lu2PoGyP"
      },
      "source": [
        "Note: The Preprocessing APIs used in the following section are experimental in TensorFlow 2.3 and subject to change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJmTiO0IYAjm"
      },
      "source": [
        "### Chuẩn hóa dữ liệu cho việc training\n",
        "\n",
        "Tiếp theo, bạn sẽ chuẩn hóa, mã hóa và vectơ hóa dữ liệu bằng cách sử dụng lớp `preprocessing.TextVectorization`. \n",
        "\n",
        "Tiêu chuẩn hóa đề cập đến việc xử lý trước văn bản, thường là loại bỏ dấu chấm câu hoặc các phần tử HTML để đơn giản hóa tập dữ liệu.  Tokenization đề cập đến việc chia nhỏ các chuỗi thành các mã thông báo (ví dụ: tách một câu thành các từ riêng lẻ, bằng cách tách trên khoảng trắng).  Vectơ hóa đề cập đến việc chuyển đổi các mã thông báo thành số để chúng có thể được đưa vào mạng nơ-ron.  Tất cả các nhiệm vụ này có thể được thực hiện với lớp này.\n",
        "\n",
        "Như bạn đã thấy ở trên, các bài đánh giá chứa các thẻ HTML khác nhau như `<br />`. Các thẻ này sẽ không bị xóa bởi trình chuẩn hóa mặc định trong lớp `TextVectorization` (chuyển đổi văn bản thành chữ thường và loại bỏ dấu chấm câu theo mặc định, nhưng không loại bỏ HTML).  Bạn sẽ viết một hàm tiêu chuẩn hóa tùy chỉnh để loại bỏ HTML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDRI_s_tX1Hk"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2d3Aw8dsUux"
      },
      "source": [
        "Tiếp theo, bạn sẽ tạo một lớp `TextVectorization`. Bạn sẽ sử dụng lớp này để chuẩn hóa, tokenize và vectơ hóa dữ liệu của chúng tôi. Bạn đặt `output_mode` thành `int` để tạo các chỉ số số nguyên duy nhất cho mỗi mã thông báo.\n",
        "\n",
        "Lưu ý rằng bạn đang sử dụng chức năng phân tách mặc định và chức năng tiêu chuẩn hóa tùy chỉnh mà bạn đã xác định ở trên.  Bạn cũng sẽ xác định một số hằng số cho mô hình, chẳng hạn như giá trị tối đa rõ ràng `sequence_length`, điều này sẽ khiến lớp đệm hoặc cắt ngắn các trình tự cho chính xác giá trị `sequence_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c76RvSzsMnX"
      },
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlFOpfF6scT6"
      },
      "source": [
        "Tiếp theo, bạn sẽ gọi `adapt` để phù hợp với trạng thái của lớp tiền xử lý với tập dữ liệu.  Điều này sẽ làm cho mô hình xây dựng một chỉ mục của chuỗi thành số nguyên."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAhdjK7AtroA"
      },
      "source": [
        "Lưu ý: điều quan trọng là chỉ sử dụng dữ liệu đào tạo của bạn khi gọi thích ứng (sử dụng bộ thử nghiệm sẽ làm rò rỉ thông tin)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH4_2ZGJsa_X"
      },
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHQVEFzNt-K_"
      },
      "source": [
        "Hãy tạo một hàm để xem kết quả của việc sử dụng lớp này để xử lý trước một số dữ liệu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCIg_T50wOCU"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XULcm6B3xQIO"
      },
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u5EX0hxyNZT"
      },
      "source": [
        "Như bạn có thể thấy ở trên, mỗi mã thông báo đã được thay thế bằng một số nguyên.  Bạn có thể tra cứu mã thông báo (chuỗi) mà mỗi số nguyên tương ứng bằng cách gọi `.get_vocabulary ()` trên lớp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRq9hTQzhVhW"
      },
      "source": [
        "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD2H6utRydGv"
      },
      "source": [
        "Bạn gần như đã sẵn sàng để đào tạo mô hình của mình.  Là bước tiền xử lý cuối cùng, bạn sẽ áp dụng lớp TextVectorization mà bạn đã tạo trước đó cho tập dữ liệu huấn luyện, xác thực và thử nghiệm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zhmpeViI1iG"
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsVQyPMizjuO"
      },
      "source": [
        "### Định cấu hình tập dữ liệu cho hiệu suất\n",
        "\n",
        "Đây là hai phương pháp quan trọng bạn nên sử dụng khi tải dữ liệu để đảm bảo rằng I / O không bị chặn.\n",
        "\n",
        "`.cache()` giữ dữ liệu trong bộ nhớ sau khi nó được tải ra khỏi đĩa.  Điều này sẽ đảm bảo tập dữ liệu không trở thành nút cổ chai trong khi đào tạo mô hình của bạn.  Nếu tập dữ liệu của bạn quá lớn để vừa với bộ nhớ, bạn cũng có thể sử dụng phương pháp này để tạo bộ đệm ẩn trên đĩa hiệu quả, hiệu quả hơn để đọc so với nhiều tệp nhỏ.\n",
        "\n",
        "`.prefetch()` chồng chéo tiền xử lý dữ liệu và thực thi mô hình trong khi đào tạo.\n",
        "\n",
        "Bạn có thể tìm hiểu thêm về cả hai phương pháp cũng như cách lưu dữ liệu vào bộ nhớ cache vào đĩa trong [data performance guide](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMcs_H7izm5m"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "### Tạo mô hình\n",
        "\n",
        "Đã đến lúc tạo mạng nơ-ron của chúng ta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkQP6in8yUBR"
      },
      "source": [
        "embedding_dim = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpKOoWgu-llD"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "Các lớp được xếp chồng lên nhau tuần tự để xây dựng bộ phân loại:\n",
        "\n",
        "1. Lớp đầu tiên là một lớp `Embedding`.  Lớp này nhận các đánh giá được mã hóa số nguyên và tìm kiếm một vectơ nhúng cho mỗi từ-chỉ mục.  Các vectơ này được học khi mô hình đào tạo.  Các vectơ thêm một thứ nguyên vào mảng đầu ra.  Các kích thước kết quả là: `(batch, sequence, embedding)`.  Để tìm hiểu thêm về cách nhúng, hãy xem [word embedding tutorial](../text/word_embeddings.ipynb).\n",
        "2. Tiếp theo, một lớp `GlobalAveragePooling1D` trả về một vectơ đầu ra có độ dài cố định cho mỗi ví dụ bằng cách lấy trung bình trên thứ nguyên trình tự.  Điều này cho phép mô hình xử lý đầu vào có độ dài thay đổi, theo cách đơn giản nhất có thể.\n",
        "3. Vectơ đầu ra có độ dài cố định này được chuyển qua một kết nối đầy đủ (`Dense`)  lớp với 16 đơn vị ẩn.\n",
        "4. Lớp cuối cùng được kết nối dày đặc với một nút đầu ra duy nhất."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### Chức năng mất mát và trình tối ưu hóa\n",
        "\n",
        "Một mô hình cần một chức năng mất mát và một trình tối ưu hóa để đào tạo.  Vì đây là một vấn đề phân loại nhị phân và mô hình xuất ra một xác suất (một lớp đơn vị với kích hoạt sigmoid), bạn sẽ sử dụng BinaryCrossentropy` loss function.\n",
        "\n",
        "Bây giờ, hãy định cấu hình mô hình để sử dụng an optimizer and a loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Bạn sẽ huấn luyện mô hình bằng cách chuyển đối tượng tập dữ liệu sang phương thức fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXSGrjWZ-llW"
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "### Đánh giá mô hình\n",
        "\n",
        "Hãy xem mô hình hoạt động như thế nào.  Hai giá trị sẽ được trả về.  Mất mát (một con số đại diện cho lỗi của chúng tôi, giá trị càng thấp càng tốt) và độ chính xác."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1iEXVTR0Z2t"
      },
      "source": [
        "Cách tiếp cận khá ngây thơ này đạt độ chính xác khoảng 86%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldbQqCw2Xc1W"
      },
      "source": [
        "### Tạo một biểu đồ về độ chính xác và mất mát theo thời gian\n",
        "\n",
        "\n",
        "`model.fit()` trả về một `History` chứa một từ điển với mọi thứ đã xảy ra trong quá trình đào tạo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YcvZsdvWfDf"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_CH32qJXruI"
      },
      "source": [
        "Có bốn mục nhập: một mục cho mỗi chỉ số được giám sát trong quá trình đào tạo và xác nhận.  Bạn có thể sử dụng chúng để lập biểu đồ về sự mất mát trong quá trình đào tạo và xác thực để so sánh, cũng như độ chính xác của quá trình đào tạo và xác nhận:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SEMeQ5YXs8z"
      },
      "source": [
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3PJemLPXwz_"
      },
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFFyCuJoXy7r"
      },
      "source": [
        "Trong biểu đồ này, các dấu chấm thể hiện sự mất mát trong quá trình huấn luyện và độ chính xác, còn các đường liền nét là sự mất xác thực và độ chính xác.\n",
        "\n",
        "Lưu ý rằng tổn thất đào tạo giảm theo từng kỷ nguyên và độ chính xác đào tạo tăng lên theo từng kỷ nguyên.  Điều này được mong đợi khi sử dụng tối ưu hóa giảm dần độ dốc — nó sẽ giảm thiểu số lượng mong muốn trên mỗi lần lặp.\n",
        "\n",
        "Đây không phải là trường hợp của việc mất xác thực và độ chính xác — chúng dường như đạt đến đỉnh điểm trước độ chính xác của quá trình huấn luyện.  Đây là một ví dụ về overfitting: mô hình hoạt động tốt hơn trên dữ liệu đào tạo so với dữ liệu mà nó chưa từng thấy trước đây.  Sau thời điểm này, mô hình tối ưu hóa quá mức và tìm hiểu các đại diện cụ thể cho dữ liệu đào tạo mà không tổng quát hóa để kiểm tra dữ liệu.\n",
        "\n",
        "Đối với trường hợp cụ thể này, bạn có thể ngăn chặn việc trang bị quá mức bằng cách dừng đào tạo khi độ chính xác xác nhận không còn tăng nữa.  Một cách để làm như vậy là sử dụng `tf.keras.callbacks.EarlyStopping` callback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-to23J3Vy5d3"
      },
      "source": [
        "## Export the model\n",
        "\n",
        "Trong đoạn mã trên, bạn đã áp dụng `TextVectorization` lớp vào tập dữ liệu trước khi cung cấp văn bản cho mô hình.  Nếu bạn muốn làm cho mô hình của mình có khả năng xử lý các chuỗi thô (ví dụ: để đơn giản hóa việc triển khai nó), bạn có thể bao gồm `TextVectorization` lớp bên trong mô hình của bạn.  Để làm như vậy, bạn có thể tạo một mô hình mới bằng cách sử dụng trọng lượng bạn vừa tập."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWXsMvryuZuq"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwQgoN88LoEF"
      },
      "source": [
        "### Suy luận về dữ liệu mới\n",
        "\n",
        "Để nhận dự đoán cho các ví dụ mới, bạn có thể chỉ cần gọi `model.predict()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW355HH5L49K"
      },
      "source": [
        "examples = [\n",
        "  \"The movie was great!\",\n",
        "  \"The movie was okay.\",\n",
        "  \"The movie was terrible...\"\n",
        "]\n",
        "\n",
        "export_model.predict(examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaxlpFWpzR6c"
      },
      "source": [
        "Bao gồm logic tiền xử lý văn bản bên trong mô hình của bạn cho phép bạn xuất một mô hình để sản xuất giúp đơn giản hóa việc triển khai và giảm khả năng [train/test skew](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew).\n",
        "\n",
        "Có một sự khác biệt về hiệu suất cần lưu ý khi chọn vị trí áp dụng lớp TextVectorization của bạn.  Sử dụng nó bên ngoài mô hình của bạn cho phép bạn xử lý CPU không đồng bộ và lưu vào bộ đệm dữ liệu của bạn khi đào tạo về GPU.  Vì vậy, nếu bạn đang đào tạo mô hình của mình trên GPU, bạn có thể muốn sử dụng tùy chọn này để có được hiệu suất tốt nhất trong khi phát triển mô hình của mình, sau đó chuyển sang bao gồm lớp `TextVectorization` bên trong mô hình của bạn khi bạn sẵn sàng chuẩn bị triển khai.\n",
        "\n",
        "Đi đến [tutorial](https://www.tensorflow.org/tutorials/keras/save_and_load) để tìm hiểu thêm về saving models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSSuci_6nCEG"
      },
      "source": [
        "## Exercise: multiclass classification on Stack Overflow questions\n",
        "\n",
        "This tutorial showed how to train a binary classifier from scratch on the IMDB dataset. As an exercise, you can modify this notebook to train a multiclass classifier to predict the tag of a programming question on [Stack Overflow](http://stackoverflow.com/).\n",
        "\n",
        "We have prepared a [dataset](http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz) for you to use containing the body of several thousand programming questions (for example, \"How can sort a dictionary by value in Python?\") posted to Stack Overflow. Each of these is labeled with exactly one tag (either Python, CSharp, JavaScript, or Java). Your task is to take a question as input, and predict the appropriate tag, in this case, Python. \n",
        "\n",
        "The dataset you will work with contains several thousand questions extracted from the much larger public Stack Overflow dataset on [BigQuery](https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow), which contains more than 17 million posts.\n",
        "\n",
        "After downloading the dataset, you will find it has a similar directory structure to the IMDB dataset you worked with previously:\n",
        "\n",
        "```\n",
        "train/\n",
        "...python/\n",
        "......0.txt\n",
        "......1.txt\n",
        "...javascript/\n",
        "......0.txt\n",
        "......1.txt\n",
        "...csharp/\n",
        "......0.txt\n",
        "......1.txt\n",
        "...java/\n",
        "......0.txt\n",
        "......1.txt\n",
        "```\n",
        "\n",
        "Note: to increase the difficulty of the classification problem, we have replaced any occurences of the words Python, CSharp, JavaScript, or Java in the programming questions with the word *blank* (as many questions contain the language they're about). \n",
        "\n",
        "To complete this exercise, you should modify this notebook to work with the Stack Overflow dataset by making the following modifications:\n",
        "\n",
        "1. At the top of your notebook, update the code that downloads the IMDB dataset with code to download the [Stack Overflow dataset](http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz) we have prepreared. As the Stack Overflow dataset has a similar directory structure, you will not need to make many modifications. \n",
        "\n",
        "1. Modify the last layer of your model to read `Dense(4)`, as there are now four output classes.\n",
        "\n",
        "1. When you compile your model, change the loss to `losses.SparseCategoricalCrossentropy`. This is the correct loss function to use for a multiclass classification problem, when the labels for each class are integers (in our case, they can be 0, *1*, *2*, or *3*).\n",
        "\n",
        "1. Once these changes are complete, you will be able to train a multiclass classifier. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0T5SIwSm7uc"
      },
      "source": [
        "## Learning more\n",
        "\n",
        "This tutorial introduced text classification from scratch. To learn more about the text classification workflow in general, we recommend reading [this guide](https://developers.google.com/machine-learning/guides/text-classification/) from Google Developers.\n"
      ]
    }
  ]
}